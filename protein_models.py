# -*- coding: utf-8 -*-
"""protein_models

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qvqcD6DkRFfamM7KhCLWmQqmiuAA4Oj_
"""

import torch
import torch.nn as nn
import lightning as L
import torchmetrics
import torchmetrics
from torchmetrics.classification import MultilabelF1Score
from torch.optim.lr_scheduler import ReduceLROnPlateau

class LightningGOModel(L.LightningModule):
    def __init__(self, model: nn.Module, learning_rate: int):
        super().__init__()

        self.learning_rate = learning_rate
        self.model = model
        self.loss_fn = nn.BCEWithLogitsLoss()
        self.train_f1 = MultilabelF1Score(num_labels = model.num_labels, threshold = 0.5 , average = 'micro')
        self.val_f1 = MultilabelF1Score(num_labels = model.num_labels, threshold = 0.5 , average = 'micro')


    def forward(self, x):
        if isinstance(self.model, GOCNN):
          x = x.view(-1,1,self.model.num_features)
        return self.model(x)

    def training_step(self, batch, batch_idx):
        X, y_true = batch
        if isinstance(self.model, GOCNN):
          X = X.view(-1,1,self.model.num_features)
        logits = self(X)
        loss = self.loss_fn(logits, y_true)
        self.log('train_loss', loss, sync_dist = True) # look up docs on log, add sync to each log
        probability = torch.sigmoid(logits)
        self.train_f1(probability, y_true)
        self.log('train_f1', self.train_f1, prog_bar = True, on_epoch= True, on_step = False, sync_dist = True)

        return loss

    def validation_step(self, batch, batch_idx):
        X, y_true = batch
        if isinstance(self.model, GOCNN):
          X = X.view(-1,1,self.model.num_features)
        logits = self(X)
        loss = self.loss_fn(logits, y_true)
        self.log('val_loss', loss, sync_dist = True)

        probability = torch.sigmoid(logits)
        self.val_f1(probability, y_true)
        self.log('val_f1', self.val_f1, prog_bar = True, sync_dist = True)

    def configure_optimizers(self):
      optimizer = torch.optim.Adam(self.parameters(), lr = self.learning_rate)
      scheduler = {
          'scheduler': ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True),
          'reduce_on_plateau': True,
          'monitor': 'val_f1',  }
      return {"optimizer": optimizer, "lr_scheduler": scheduler}

class GOCNN(nn.Module):
  def __init__(self, num_features:int, num_labels:int, conv_kernel:int,
               conv_stride:int, pool_kernel: int, dropout= None):

    super().__init__()
    self.num_features = num_features
    self.num_labels = num_labels
    if dropout is not None and 0 < dropout < 1:
      self.dropout = nn.Dropout(dropout)

    self.conv1 = nn.Sequential(
        nn.Conv1d(1, 64, kernel_size = conv_kernel, stride = conv_stride),
        nn.BatchNorm1d(64),
        nn.ReLU())

    self.maxpool1 = nn.MaxPool1d(kernel_size = pool_kernel)

    self.conv2 = nn.Sequential(
        nn.Conv1d(64, 128, kernel_size = conv_kernel, stride = conv_stride),
        nn.BatchNorm1d(128),
        nn.ReLU())

    self.maxpool2 = nn.MaxPool1d(kernel_size = pool_kernel)


    self.flattened_size = self.unraveler(self.num_features)
    self.fc_layers = nn.Sequential(
        nn.Flatten(),
        nn.Linear(self.flattened_size, 1024),
        nn.BatchNorm1d(1024),
        nn.ReLU())
    self.lin = nn.Linear(1024, self.num_labels)



  def unraveler(self, num_features):
    dummy = torch.rand(1,1, num_features)
    output = self.conv2(self.conv1(dummy)).numel()
    return output

  def forward(self, x):
    x = self.conv1(x)
    if hasattr(self, 'dropout'):
      x = self.dropout(x)

    x = self.conv2(x)
    if hasattr(self, 'dropout'):
      x = self.dropout(x)

    x = self.fc_layers(x)
    if hasattr(self, 'dropout'):
      x = self.dropout(x)

    x= self.lin(x)

    return x

class GOFC_Small(nn.Module):
  def __init__(self, num_features, num_labels, dropout =0):
    super().__init__()
    self.num_features = num_features
    self.num_labels = num_labels
    self.all_layers = nn.Sequential(
        nn.Linear(num_features, 1024),
        nn.ReLU(),
        nn.Dropout(dropout),
        nn.Linear(1024, 512),
        nn.ReLU(),
        nn.Dropout(dropout),
        nn.Linear(512, 256),
        nn.ReLU(),
        nn.Dropout(dropout),
        nn.Linear(256, self.num_labels))
  def forward(self, x):
        return self.all_layers(x)

class GOFC_Large(nn.Module):
  def __init__(self, num_features:int, num_labels:int,  dropout= 0):
    super().__init__()
    self.num_features = num_features
    self.num_labels = num_labels
    self.all_layers = nn.Sequential(
        nn.Linear(num_features, 1024),
        nn.ReLU(),
        nn.Dropout(dropout),
        nn.Linear(1024, 1024),
        nn.ReLU(),
        nn.Dropout(dropout),
        nn.Linear(1024, 512),
        nn.ReLU(),
        nn.Dropout(dropout),
        nn.Linear(512, self.num_labels)
    )
  def forward(self, x):
    return self.all_layers(x)

